//  ================================================================
//  Created by Gregory Kramida on 10/1/19.
//  Copyright (c) 2019 Gregory Kramida
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.
//  ================================================================

#include <device_atomic_functions.h>
#include "ITMSceneStatisticsCalculator_CUDA.h"
#include "../../../ITMLibSettings.h"
#include "../../../../Engines/Traversal/CUDA/ITMSceneTraversal_CUDA_VoxelBlockHash.h"
#include "../../../../Engines/Traversal/CUDA/ITMSceneTraversal_CUDA_PlainVoxelArray.h"

using namespace ITMLib;

namespace {
//CUDA kernel declaration

__global__ void computeVoxelBounds(const ITMHashEntry* hashTable, Vector6i* bounds, int noTotalEntries);

} // end anonymous namespace

template<typename TVoxel, typename TIndex>
struct ComputeVoxelBoundsFunctor;

template<typename TVoxel>
struct ComputeVoxelBoundsFunctor<TVoxel, ITMVoxelBlockHash> {
	static Vector6i Compute(const ITMVoxelVolume<TVoxel, ITMVoxelBlockHash>* scene) {

		Vector6i bounds = Vector6i(0);
		const TVoxel* voxelBlocks = scene->localVBA.GetVoxelBlocks();
		const ITMHashEntry* hashTable = scene->index.GetEntries();
		int noTotalEntries = scene->index.noTotalEntries;

		dim3 cudaBlockSize(256, 1);
		dim3 cudaGridSize((int) ceil((float) noTotalEntries / (float) cudaBlockSize.x));

		Vector6i* boundsCuda = nullptr;

		ORcudaSafeCall(cudaMalloc((void**) &boundsCuda, sizeof(Vector6i)));
		ORcudaSafeCall(cudaMemcpy(boundsCuda, (void*) &bounds, sizeof(Vector6i), cudaMemcpyHostToDevice));

		computeVoxelBounds << < cudaGridSize, cudaBlockSize >> > (hashTable, boundsCuda, noTotalEntries);
		ORcudaKernelCheck;

		ORcudaSafeCall(cudaMemcpy((void*) &bounds, boundsCuda, sizeof(Vector6i), cudaMemcpyDeviceToHost));
		ORcudaSafeCall(cudaFree(boundsCuda));
		return bounds;
	}
};

template<typename TVoxel>
struct ComputeVoxelBoundsFunctor<TVoxel, ITMPlainVoxelArray> {
	static Vector6i Compute(const ITMVoxelVolume<TVoxel, ITMPlainVoxelArray>* scene) {
		const ITMPlainVoxelArray::IndexData* indexData = scene->index.getIndexData();
		return Vector6i(indexData->offset.x, indexData->offset.y, indexData->offset.z,
		                indexData->offset.x + indexData->size.x, indexData->offset.y + indexData->size.y,
		                indexData->offset.z + indexData->size.z);
	}
};

template<typename TVoxel, typename TIndex>
Vector6i
ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeVoxelBounds(const ITMVoxelVolume<TVoxel, TIndex>* scene) {
	return ComputeVoxelBoundsFunctor<TVoxel, TIndex>::Compute(scene);
}

template<typename TVoxel, typename TIndex>
int
ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeAllocatedVoxelCount(ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
std::vector<int>
ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::GetFilledHashBlockIds(ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
int ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeAllocatedHashBlockCount(
		ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}


template<bool hasSemanticInformation, typename TVoxel, typename TIndex>
struct ComputeNonTruncatedVoxelCountFunctor;

template<class TVoxel, typename TIndex>
struct ComputeNonTruncatedVoxelCountFunctor<false, TVoxel, TIndex> {
	static int compute(ITMVoxelVolume<TVoxel, TIndex>* scene) {
		DIEWITHEXCEPTION("Voxels need to have semantic information to be marked as truncated or non-truncated.");
	}
};
template<class TVoxel, typename TIndex>
struct ComputeNonTruncatedVoxelCountFunctor<true, TVoxel, TIndex> {
	//not an optimal way (reduction would be much faster), but works -- we don't need this uber-fast
	__host__
	static int compute(ITMVoxelVolume<TVoxel, TIndex>* scene) {
		ComputeNonTruncatedVoxelCountFunctor instance;

		ITMSceneTraversalEngine<TVoxel, TIndex, ITMLibSettings::DEVICE_CUDA>::
		        VoxelTraversal(scene,instance);
		int count = 0;
		cudaMemcpy(&count, instance.count_device, sizeof(int), cudaMemcpyDeviceToHost);
		return count;
	}

	int* count_device = nullptr;
	__device__
	void operator()(TVoxel& voxel) {
		atomicAdd(count_device, (int) (voxel.flags == ITMLib::VOXEL_NONTRUNCATED));
	}
	ComputeNonTruncatedVoxelCountFunctor(){
		ORcudaSafeCall(cudaMalloc((void**)&count_device, sizeof(int)));
		ORcudaSafeCall(cudaMemset(count_device, 0, sizeof(int)));
	}
	~ComputeNonTruncatedVoxelCountFunctor(){
		ORcudaSafeCall(cudaFree(count_device));
	}
};

template<typename TVoxel, typename TIndex>
int ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeNonTruncatedVoxelCount(
		ITMVoxelVolume<TVoxel, TIndex>* scene) {
	return ComputeNonTruncatedVoxelCountFunctor<TVoxel::hasSemanticInformation, TVoxel, TIndex>::compute(scene);
}

template<typename TVoxel, typename TIndex>
int ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeVoxelWithNonZeroSdfCount(
		ITMVoxelVolume<TVoxel, TIndex>* scene, float value) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
double ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeNonTruncatedVoxelAbsSdfSum(
		ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
double ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeTruncatedVoxelAbsSdfSum(
		ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}


template<typename TVoxel, typename TIndex>
float ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::FindMaxGradient0LengthAndPosition(
		ITMVoxelVolume<TVoxel, TIndex>* scene, Vector3i& positionOut) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
float ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::FindMaxGradient1LengthAndPosition(
		ITMVoxelVolume<TVoxel, TIndex>* scene, Vector3i& positionOut) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

template<typename TVoxel, typename TIndex>
unsigned  int ITMSceneStatisticsCalculator_CUDA<TVoxel, TIndex>::ComputeAlteredVoxelCount(ITMVoxelVolume<TVoxel, TIndex>* scene) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
}

namespace {
// CUDA kernel implementations

__global__ void computeVoxelBounds(const ITMHashEntry* hashTable, Vector6i* bounds, int noTotalEntries) {
	int hash = threadIdx.x + blockIdx.x * blockDim.x;
	if (hash >= noTotalEntries) return;

	const ITMHashEntry& hashEntry = hashTable[hash];
	if (hashEntry.ptr < 0) return;

	Vector3i hashEntryPosVoxels = hashEntry.pos.toInt() * SDF_BLOCK_SIZE;

	atomicMin(&(bounds->min_x), hashEntryPosVoxels.x);
	atomicMin(&(bounds->min_y), hashEntryPosVoxels.y);
	atomicMin(&(bounds->min_z), hashEntryPosVoxels.z);
	atomicMax(&(bounds->max_x), hashEntryPosVoxels.x + SDF_BLOCK_SIZE);
	atomicMax(&(bounds->max_y), hashEntryPosVoxels.y + SDF_BLOCK_SIZE);
	atomicMax(&(bounds->max_z), hashEntryPosVoxels.z + SDF_BLOCK_SIZE);
}

} // end anonymous namespace