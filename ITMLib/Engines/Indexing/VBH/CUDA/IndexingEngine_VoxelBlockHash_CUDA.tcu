//  ================================================================
//  Created by Gregory Kramida on 11/1/19.
//  Copyright (c) 2019 Gregory Kramida
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.
//  ================================================================
#pragma once

//local
#include "IndexingEngine_VoxelBlockHash_CUDA.h"
#include "IndexingEngine_VoxelBlockHash_CUDA_Kernels.h"
#include "../../Interface/IndexingEngine.h"
#include "../CPU/IndexingEngine_VoxelBlockHash_CPU.h"
#include "../../../../../ORUtils/PlatformIndependence.h"
#include "../../Shared/IndexingEngine_Functors.h"
#include "../../../Traversal/Interface/HashTableTraversal.h"
#include "../../../../Engines/Analytics/AnalyticsEngine.h"
#include "../../../../Engines/Analytics/AnalyticsEngineFactory.h"
#include "../../../../Utils/Collections/MemoryBlock_StdContainer_Convertions.h"


using namespace ITMLib;


namespace ITMLib{

namespace internal{

template<typename TVoxel>
HashEntry IndexingEngine_VoxelBlockHash_MemoryDeviceTypeSpecialized<MEMORYDEVICE_CUDA, TVoxel>::FindHashEntry(const VoxelBlockHash& index, const Vector3s& coordinates, int& hash_code){
	const HashEntry* entries = index.GetEntries();
	ORUtils::MemoryBlock<int> hashCode(1, true, true);
	ORUtils::MemoryBlock<HashEntry> hashEntry(1, true, true);
	findHashEntry_device <<< 1, 1 >>>(hashEntry.GetData(MEMORYDEVICE_CUDA), entries, coordinates, hashCode.GetData(
			MEMORYDEVICE_CUDA));
	ORcudaKernelCheck;
	hashEntry.UpdateHostFromDevice();
	hashCode.UpdateHostFromDevice();
	hash_code = *hashCode.GetData(MEMORYDEVICE_CPU);
	if (hash_code == -1) {
		return {Vector3s(0, 0, 0), 0, -2, (uchar)0, (unsigned short)0};
	} else {
		HashEntry entry = *hashEntry.GetData(MEMORYDEVICE_CPU);
		return entry;
	}
}
template<typename TVoxel>
bool IndexingEngine_VoxelBlockHash_MemoryDeviceTypeSpecialized<MEMORYDEVICE_CUDA, TVoxel>::AllocateHashBlockAt(VoxelVolume<TVoxel, VoxelBlockHash>* volume, Vector3s at, int& hash_code){
	HashEntry* hash_table = volume->index.GetEntries();

	ORUtils::MemoryBlock<SingleHashAllocationData> data(1, true, true);

	SingleHashAllocationData* data_CPU = data.GetData(MEMORYDEVICE_CPU);
	data_CPU->last_free_allocation_block_id = volume->index.GetLastFreeBlockListId();
	data_CPU->last_free_excess_entry_id = volume->index.GetLastFreeExcessListId();
	data_CPU->utilized_hash_code_count = volume->index.GetUtilizedBlockCount();
	data_CPU->hash_code = -1;
	data_CPU->success = false;
	data.UpdateDeviceFromHost();

	int* voxel_allocation_list = volume->index.GetBlockAllocationList();
	int* excess_allocation_list = volume->index.GetExcessEntryList();
	int* utilized_hash_codes = volume->index.GetUtilizedBlockHashCodes();

	allocateHashEntry_device <<< 1, 1 >>>(
			data.GetData(MEMORYDEVICE_CUDA), at, hash_table,
			voxel_allocation_list, excess_allocation_list, utilized_hash_codes
	);

	ORcudaKernelCheck;
	data.UpdateHostFromDevice();

	if (!data_CPU->success) {
		return false;
	}

	hash_code = data_CPU->hash_code;

	volume->index.SetLastFreeBlockListId(data_CPU->last_free_allocation_block_id);
	volume->index.SetLastFreeExcessListId(data_CPU->last_free_excess_entry_id);
	volume->index.SetUtilizedBlockCount(data_CPU->utilized_hash_code_count);
	return true;
}
template<typename TVoxel>
void IndexingEngine_VoxelBlockHash_MemoryDeviceTypeSpecialized<MEMORYDEVICE_CUDA, TVoxel>::RebuildVisibleBlockList(
		VoxelVolume<TVoxel, VoxelBlockHash>* volume, const View* view, const Matrix4f& depth_camera_matrix) {

	// ** volume data **
	const int hash_entry_count = volume->index.hash_entry_count;
	HashBlockVisibility* block_visibility_types = volume->index.GetBlockVisibilityTypes();
	int* visible_block_hash_codes = volume->index.GetVisibleBlockHashCodes();
	HashEntry* hash_table = volume->index.GetEntries();
	const bool use_swapping = volume->SwappingEnabled();
	HashSwapState* swapStates = volume->SwappingEnabled() ? volume->global_cache.GetSwapStates(false) : 0;

	// ** view data **
	Vector4f depthCameraProjectionParameters = view->calibration_information.intrinsics_d.projectionParamsSimple.all;
	Vector2i depthImgSize = view->depth.dimensions;
	float voxelSize = volume->GetParameters().voxel_size;


	// ** CUDA data **
	ORUtils::MemoryBlock<int> visible_block_count(1, true, true);
	dim3 cuda_block_size(256, 1);
	dim3 cuda_grid_size((int) ceil((float) hash_entry_count / (float) cuda_block_size.x));

	if (use_swapping) {
		buildVisibilityList_device<true> <<< cuda_grid_size, cuda_block_size >>>
				(hash_table, swapStates, hash_entry_count, visible_block_hash_codes,
				 visible_block_count.GetData(MEMORYDEVICE_CUDA), block_visibility_types, depth_camera_matrix,
				 depthCameraProjectionParameters, depthImgSize, voxelSize);
		ORcudaKernelCheck;
	} else {
		buildVisibilityList_device<false> <<< cuda_grid_size, cuda_block_size >>>
				(hash_table, swapStates, hash_entry_count, visible_block_hash_codes,
				 visible_block_count.GetData(MEMORYDEVICE_CUDA), block_visibility_types, depth_camera_matrix,
				 depthCameraProjectionParameters, depthImgSize, voxelSize);
		ORcudaKernelCheck;
	}
	visible_block_count.UpdateHostFromDevice();
	volume->index.SetVisibleBlockCount(*visible_block_count.GetData(MEMORYDEVICE_CPU));
}

template<typename TVoxelTarget, typename TVoxelSource>
void AllocateUsingOtherVolume_OffsetAndBounded_Executor<MEMORYDEVICE_CUDA, TVoxelTarget, TVoxelSource>::Execute(
		VoxelVolume<TVoxelTarget, VoxelBlockHash>* target_volume,
		VoxelVolume<TVoxelSource, VoxelBlockHash>* source_volume, const Extent3Di& source_bounds,
		const Vector3i& target_offset) {

	IndexingEngine<TVoxelTarget, VoxelBlockHash, MEMORYDEVICE_CUDA>& target_indexer =
			IndexingEngine<TVoxelTarget, VoxelBlockHash, MEMORYDEVICE_CUDA>::Instance();

	HashEntry* target_hash_table = target_volume->index.GetEntries();
	const HashEntry* source_hash_table = source_volume->index.GetEntries();

	Vector6i target_bounds(source_bounds.min_x + target_offset.x,
	                       source_bounds.min_y + target_offset.y,
	                       source_bounds.min_z + target_offset.z,
	                       source_bounds.max_x + target_offset.x,
	                       source_bounds.max_y + target_offset.y,
	                       source_bounds.max_z + target_offset.z);

	Vector3i min_target_block_coordinate(
			static_cast<int>(floor(static_cast<float>(target_bounds.min_x) / VOXEL_BLOCK_SIZE)),
			static_cast<int>(floor(static_cast<float>(target_bounds.min_y) / VOXEL_BLOCK_SIZE)),
			static_cast<int>(floor(static_cast<float>(target_bounds.min_z) / VOXEL_BLOCK_SIZE)));
	Vector3i max_target_block_coord(
			static_cast<int>(ceil(static_cast<float>(target_bounds.max_x) / VOXEL_BLOCK_SIZE)),
			static_cast<int>(ceil(static_cast<float>(target_bounds.max_y) / VOXEL_BLOCK_SIZE)),
			static_cast<int>(ceil(static_cast<float>(target_bounds.max_z) / VOXEL_BLOCK_SIZE)));
	Vector3i target_block_range(max_target_block_coord.x + 1 - min_target_block_coordinate.x,
	                            max_target_block_coord.y + 1 - min_target_block_coordinate.y,
	                            max_target_block_coord.z + 1 - min_target_block_coordinate.z);

	HashEntryAllocationState* hash_entry_allocation_states = target_volume->index.GetHashEntryAllocationStates();
	Vector3s* hash_block_coordinates = target_volume->index.GetAllocationBlockCoordinates();

	ORUtils::MemoryBlock<Vector3s> colliding_block_positions(target_volume->index.hash_entry_count, MEMORYDEVICE_CUDA);
	Vector3s* colliding_block_positions_device = colliding_block_positions.GetData(MEMORYDEVICE_CUDA);
	ORUtils::MemoryBlock<int> collidng_block_count(1, true, true);
	ORUtils::MemoryBlock<bool> unresolvable_collision_encountered(1, true, true);


	Vector6i inverse_offset_block_range;
	ComputeVoxelBlockOffsetRange(-target_offset, inverse_offset_block_range);
	dim3 cudaBlockSizeMK(8, 8, 8);
	dim3 gridSizeMK((int) ceil((float) target_block_range.x / (float) cudaBlockSizeMK.x),
	                (int) ceil((float) target_block_range.y / (float) cudaBlockSizeMK.x),
	                (int) ceil((float) target_block_range.z / (float) cudaBlockSizeMK.x));

	do {
		target_volume->index.ClearHashEntryAllocationStates();
		*collidng_block_count.GetData(MEMORYDEVICE_CPU) = 0;
		collidng_block_count.UpdateDeviceFromHost();
		*unresolvable_collision_encountered.GetData(MEMORYDEVICE_CPU) = false;
		unresolvable_collision_encountered.UpdateDeviceFromHost();

		determineTargetAllocationForOffsetCopy_device <<< gridSizeMK, cudaBlockSizeMK >>>(
				target_hash_table, source_hash_table, hash_entry_allocation_states, hash_block_coordinates,
				target_bounds, inverse_offset_block_range, target_block_range, min_target_block_coordinate,
				colliding_block_positions_device, collidng_block_count.GetData(MEMORYDEVICE_CUDA),
				unresolvable_collision_encountered.GetData(MEMORYDEVICE_CUDA));
		ORcudaKernelCheck;
		collidng_block_count.UpdateHostFromDevice();
		target_indexer.AllocateHashEntriesUsingAllocationStateList(target_volume);
		target_indexer.AllocateBlockList(target_volume, colliding_block_positions,
		                                 *collidng_block_count.GetData(MEMORYDEVICE_CPU));
		unresolvable_collision_encountered.UpdateHostFromDevice();
	} while (*unresolvable_collision_encountered.GetData(MEMORYDEVICE_CPU));
}

} // namespace internal

} // namespace ITMLib