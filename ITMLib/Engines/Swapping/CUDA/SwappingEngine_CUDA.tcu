// Copyright 2014-2017 Oxford University Innovation Limited and the authors of InfiniTAM

#include "SwappingEngine_CUDA.h"

#include "../Shared/SwappingEngine_Shared.h"
#include "../../../Utils/CUDAUtils.h"

using namespace ITMLib;

namespace
{

	__global__ void buildListToSwapIn_device(int *neededEntryIDs, int *noNeededEntries, ITMHashSwapState *swapStates, int noTotalEntries);

	template<class TVoxel>
	__global__ void integrateOldIntoActiveData_device(TVoxel *localVBA, ITMHashSwapState *swapStates, TVoxel *syncedVoxelBlocks_local,
	                                                  int *neededEntryIDs_local, HashEntry *hashTable, int maxW);

	__global__ void buildListToSwapOut_device(int *neededEntryIDs, int *noNeededEntries, ITMHashSwapState *swapStates,
	                                          HashEntry *hashTable, HashBlockVisibility* entriesVisibleType, int noTotalEntries);

	__global__ void buildListToClean_device(int *neededEntryIDs, int *noNeededEntries, HashEntry *hashTable, HashBlockVisibility* blockVisibilityTypes, int noTotalEntries);

	template<class TVoxel>
	__global__ void cleanMemory_device(int *voxelAllocationList, int *noAllocatedVoxelEntries, ITMHashSwapState *swapStates,
	                                   HashEntry *hashTable, TVoxel *localVBA, int *neededEntryIDs_local, int neededEntryCount, const int voxelBlockCount);

	template<class TVoxel>
	__global__ void cleanMemory_device(int *voxelAllocationList, int *noAllocatedVoxelEntries,
	                                   HashEntry *hashTable, TVoxel *localVBA, int *neededEntryIDs_local, int neededEntryCount, const int voxelBlockCount);

	template<class TVoxel>
	__global__ void cleanVBA(int *neededEntryIDs_local, HashEntry *hashTable, TVoxel *localVBA);

	template<class TVoxel>
	__global__ void moveActiveDataToTransferBuffer_device(TVoxel *syncedVoxelBlocks_local, bool *hasSyncedData_local,
	                                                      int *neededEntryIDs_local, HashEntry *hashTable, TVoxel *localVBA);

}

template<class TVoxel>
SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::SwappingEngine_CUDA(const VoxelBlockHash& index)
{
	ORcudaSafeCall(cudaMalloc((void**)&noAllocatedVoxelEntries_device, sizeof(int)));
	ORcudaSafeCall(cudaMalloc((void**)&noNeededEntries_device, sizeof(int)));
	ORcudaSafeCall(cudaMalloc((void**)&entriesToClean_device, index.voxel_block_count * sizeof(int)));
}

template<class TVoxel>
SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::~SwappingEngine_CUDA()
{
	ORcudaSafeCall(cudaFree(noAllocatedVoxelEntries_device));
	ORcudaSafeCall(cudaFree(noNeededEntries_device));
	ORcudaSafeCall(cudaFree(entriesToClean_device));
}

template<class TVoxel>
int SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::LoadFromGlobalMemory(VoxelVolume<TVoxel, VoxelBlockHash> *scene)
{
	GlobalCache<TVoxel, VoxelBlockHash> *globalCache = scene->global_cache;

	ITMHashSwapState *swapStates = globalCache->GetSwapStates(true);

	TVoxel *syncedVoxelBlocks_local = globalCache->GetSyncedVoxelBlocks(true);
	bool *hasSyncedData_local = globalCache->GetHasSyncedData(true);
	int *neededEntryIDs_local = globalCache->GetNeededEntryIDs(true);

	TVoxel *syncedVoxelBlocks_global = globalCache->GetSyncedVoxelBlocks(false);
	bool *hasSyncedData_global = globalCache->GetHasSyncedData(false);
	int *neededEntryIDs_global = globalCache->GetNeededEntryIDs(false);

	dim3 blockSize(256);
	dim3 gridSize((int)ceil((float)scene->index.hash_entry_count / (float)blockSize.x));

	ORcudaSafeCall(cudaMemset(noNeededEntries_device, 0, sizeof(int)));

	buildListToSwapIn_device <<<gridSize, blockSize >>>(neededEntryIDs_local, noNeededEntries_device, swapStates,
		scene->global_cache->hashEntryCount);
	ORcudaKernelCheck;

	int neededHashEntryCount;
	ORcudaSafeCall(cudaMemcpy(&neededHashEntryCount, noNeededEntries_device, sizeof(int), cudaMemcpyDeviceToHost));

	if (neededHashEntryCount > 0)
	{
		neededHashEntryCount = ORUTILS_MIN(neededHashEntryCount, SWAP_OPERATION_BLOCK_COUNT);
		ORcudaSafeCall(cudaMemcpy(neededEntryIDs_global, neededEntryIDs_local, sizeof(int) * neededHashEntryCount, cudaMemcpyDeviceToHost));

		memset(syncedVoxelBlocks_global, 0, neededHashEntryCount * VOXEL_BLOCK_SIZE3 * sizeof(TVoxel));
		memset(hasSyncedData_global, 0, neededHashEntryCount * sizeof(bool));
		for (int i = 0; i < neededHashEntryCount; i++)
		{
			int entryId = neededEntryIDs_global[i];

			if (globalCache->HasStoredData(entryId))
			{
				hasSyncedData_global[i] = true;
				memcpy(syncedVoxelBlocks_global + i * VOXEL_BLOCK_SIZE3, globalCache->GetStoredVoxelBlock(entryId), VOXEL_BLOCK_SIZE3 * sizeof(TVoxel));
			}
		}

		ORcudaSafeCall(cudaMemcpy(hasSyncedData_local, hasSyncedData_global, sizeof(bool) * neededHashEntryCount, cudaMemcpyHostToDevice));
		ORcudaSafeCall(cudaMemcpy(syncedVoxelBlocks_local, syncedVoxelBlocks_global, sizeof(TVoxel) * VOXEL_BLOCK_SIZE3 * neededHashEntryCount, cudaMemcpyHostToDevice));
	}

	return neededHashEntryCount;
}

template<class TVoxel>
void SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::IntegrateGlobalIntoLocal(VoxelVolume<TVoxel, VoxelBlockHash> *scene, RenderState *renderState)
{
	GlobalCache<TVoxel, VoxelBlockHash>* globalCache = scene->global_cache;

	HashEntry *hashTable = scene->index.GetEntries();

	ITMHashSwapState *swapStates = globalCache->GetSwapStates(true);

	TVoxel *syncedVoxelBlocks_local = globalCache->GetSyncedVoxelBlocks(true);
	int *neededEntryIDs_local = globalCache->GetNeededEntryIDs(true);

	TVoxel *localVBA = scene->GetVoxelBlocks();

	int noNeededEntries = this->LoadFromGlobalMemory(scene);

	int maxW = scene->parameters->max_integration_weight;

	if (noNeededEntries > 0) {
		dim3 blockSize(VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE);
		dim3 gridSize(noNeededEntries);

		integrateOldIntoActiveData_device <<<gridSize, blockSize >>>(localVBA, swapStates, syncedVoxelBlocks_local,
			neededEntryIDs_local, hashTable, maxW);
		ORcudaKernelCheck;
	}
}

template<class TVoxel>
void SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::SaveToGlobalMemory(VoxelVolume<TVoxel, VoxelBlockHash> *scene, RenderState *renderState)
{
	GlobalCache<TVoxel, VoxelBlockHash>* globalCache = scene->global_cache;

	ITMHashSwapState *swapStates = globalCache->GetSwapStates(true);

	HashEntry *hashTable = scene->index.GetEntries();
	HashBlockVisibility* blockVisibilityTypes = scene->index.GetBlockVisibilityTypes();

	TVoxel *syncedVoxelBlocks_local = globalCache->GetSyncedVoxelBlocks(true);
	bool *hasSyncedData_local = globalCache->GetHasSyncedData(true);
	int *neededEntryIDs_local = globalCache->GetNeededEntryIDs(true);

	TVoxel *syncedVoxelBlocks_global = globalCache->GetSyncedVoxelBlocks(false);
	bool *hasSyncedData_global = globalCache->GetHasSyncedData(false);
	int *neededEntryIDs_global = globalCache->GetNeededEntryIDs(false);

	TVoxel *localVBA = scene->GetVoxelBlocks();
	int *voxelAllocationList = scene->index.GetBlockAllocationList();

	int hashEntryCount = globalCache->hashEntryCount;

	dim3 blockSize, gridSize;
	int noNeededEntries;

	{
		blockSize = dim3(256);
		gridSize = dim3((int)ceil((float)scene->index.hash_entry_count / (float)blockSize.x));

		ORcudaSafeCall(cudaMemset(noNeededEntries_device, 0, sizeof(int)));

		buildListToSwapOut_device <<<gridSize, blockSize >>>(neededEntryIDs_local, noNeededEntries_device, swapStates,
			hashTable, blockVisibilityTypes, hashEntryCount);
		ORcudaKernelCheck;

		ORcudaSafeCall(cudaMemcpy(&noNeededEntries, noNeededEntries_device, sizeof(int), cudaMemcpyDeviceToHost));
	}

	if (noNeededEntries > 0)
	{
		noNeededEntries = ORUTILS_MIN(noNeededEntries, SWAP_OPERATION_BLOCK_COUNT);
		{
			blockSize = dim3(VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE);
			gridSize = dim3(noNeededEntries);

			moveActiveDataToTransferBuffer_device <<<gridSize, blockSize >>>(syncedVoxelBlocks_local, hasSyncedData_local,
				neededEntryIDs_local, hashTable, localVBA);
			ORcudaKernelCheck;
		}

		{
			blockSize = dim3(256);
			gridSize = dim3((int)ceil((float)noNeededEntries / (float)blockSize.x));
			int last_free_block_id = scene->index.GetLastFreeBlockListId();
			ORcudaSafeCall(cudaMemcpy(noAllocatedVoxelEntries_device, &last_free_block_id, sizeof(int), cudaMemcpyHostToDevice));

			cleanMemory_device <<<gridSize, blockSize >>>(voxelAllocationList, noAllocatedVoxelEntries_device, swapStates, hashTable, localVBA,
				neededEntryIDs_local, noNeededEntries, scene->index.voxel_block_count);
			ORcudaKernelCheck;
			last_free_block_id = scene->index.GetLastFreeBlockListId();
			ORcudaSafeCall(cudaMemcpy(&last_free_block_id, noAllocatedVoxelEntries_device, sizeof(int), cudaMemcpyDeviceToHost));
			scene->index.SetLastFreeBlockListId(ORUTILS_MAX(scene->index.GetLastFreeBlockListId(), 0));
			scene->index.SetLastFreeBlockListId(ORUTILS_MIN(scene->index.GetLastFreeBlockListId(), scene->index.voxel_block_count));
		}

		ORcudaSafeCall(cudaMemcpy(neededEntryIDs_global, neededEntryIDs_local, sizeof(int) * noNeededEntries, cudaMemcpyDeviceToHost));
		ORcudaSafeCall(cudaMemcpy(hasSyncedData_global, hasSyncedData_local, sizeof(bool) * noNeededEntries, cudaMemcpyDeviceToHost));
		ORcudaSafeCall(cudaMemcpy(syncedVoxelBlocks_global, syncedVoxelBlocks_local, sizeof(TVoxel) * VOXEL_BLOCK_SIZE3 * noNeededEntries, cudaMemcpyDeviceToHost));

		for (int entryId = 0; entryId < noNeededEntries; entryId++)
		{
			if (hasSyncedData_global[entryId])
				globalCache->SetStoredData(neededEntryIDs_global[entryId], syncedVoxelBlocks_global + entryId * VOXEL_BLOCK_SIZE3);
		}
	}
}

template<class TVoxel>
void SwappingEngine_CUDA<TVoxel, VoxelBlockHash>::CleanLocalMemory(VoxelVolume<TVoxel, VoxelBlockHash> *scene, RenderState *renderState)
{
	HashEntry *hashTable = scene->index.GetEntries();
	HashBlockVisibility* blockVisibilityTypes = scene->index.GetBlockVisibilityTypes();

	TVoxel *localVBA = scene->GetVoxelBlocks();
	int *voxelAllocationList = scene->index.GetBlockAllocationList();

	dim3 blockSize, gridSize;
	int noNeededEntries;

	{
		blockSize = dim3(256);
		gridSize = dim3((int)ceil((float)scene->index.hash_entry_count / (float)blockSize.x));

		ORcudaSafeCall(cudaMemset(noNeededEntries_device, 0, sizeof(int)));

		buildListToClean_device <<<gridSize, blockSize >>>(entriesToClean_device, noNeededEntries_device, hashTable, blockVisibilityTypes, scene->index.hash_entry_count);

		ORcudaSafeCall(cudaMemcpy(&noNeededEntries, noNeededEntries_device, sizeof(int), cudaMemcpyDeviceToHost));
	}
	
	if (noNeededEntries > 0)
	{
		noNeededEntries = ORUTILS_MIN(noNeededEntries, SWAP_OPERATION_BLOCK_COUNT);
		{
			blockSize = dim3(VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE);
			gridSize = dim3(noNeededEntries);

			cleanVBA <<<gridSize, blockSize >>>(entriesToClean_device, hashTable, localVBA);
		}

		{
			blockSize = dim3(256);
			gridSize = dim3((int)ceil((float)noNeededEntries / (float)blockSize.x));
			int last_free_block_id = scene->index.GetLastFreeBlockListId();
			ORcudaSafeCall(cudaMemcpy(noAllocatedVoxelEntries_device, &last_free_block_id, sizeof(int), cudaMemcpyHostToDevice));

			cleanMemory_device <<<gridSize, blockSize >>>(voxelAllocationList, noAllocatedVoxelEntries_device, hashTable, localVBA, entriesToClean_device, noNeededEntries, scene->index.voxel_block_count);

			last_free_block_id = scene->index.GetLastFreeBlockListId();
			ORcudaSafeCall(cudaMemcpy(&last_free_block_id, noAllocatedVoxelEntries_device, sizeof(int), cudaMemcpyDeviceToHost));
			scene->index.SetLastFreeBlockListId(ORUTILS_MAX(scene->index.GetLastFreeBlockListId(), 0));
			scene->index.SetLastFreeBlockListId(ORUTILS_MIN(scene->index.GetLastFreeBlockListId(), scene->index.voxel_block_count));
		}
	}
}

namespace
{
	__global__ void buildListToSwapIn_device(int *neededEntryIDs, int *noNeededEntries, ITMHashSwapState *swapStates, int noTotalEntries)
	{
		int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
		if (targetIdx > noTotalEntries - 1) return;

		__shared__ bool shouldPrefix;

		shouldPrefix = false;
		__syncthreads();

		bool isNeededId = (swapStates[targetIdx].state == 1);

		if (isNeededId) shouldPrefix = true;
		__syncthreads();

		if (shouldPrefix)
		{
			int offset = computePrefixSum_device<int>(isNeededId, noNeededEntries, blockDim.x * blockDim.y, threadIdx.x);
			if (offset != -1 && offset < SWAP_OPERATION_BLOCK_COUNT) neededEntryIDs[offset] = targetIdx;
		}
	}

	__global__ void buildListToSwapOut_device(int *neededEntryIDs, int *noNeededEntries, ITMHashSwapState *swapStates,
	                                          HashEntry *hashTable, HashBlockVisibility* blockVisibilityTypes,
	                                          int hashEntryCount)
	{
		int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
		if (targetIdx > hashEntryCount - 1) return;

		__shared__ bool shouldPrefix;

		shouldPrefix = false;
		__syncthreads();

		ITMHashSwapState &swapState = swapStates[targetIdx];

		bool isNeededId = (swapState.state == 2 &&
			hashTable[targetIdx].ptr >= 0 && blockVisibilityTypes[targetIdx] == INVISIBLE);

		if (isNeededId) shouldPrefix = true;
		__syncthreads();

		if (shouldPrefix)
		{
			int offset = computePrefixSum_device<int>(isNeededId, noNeededEntries, blockDim.x * blockDim.y, threadIdx.x);
			if (offset != -1 && offset < SWAP_OPERATION_BLOCK_COUNT) neededEntryIDs[offset] = targetIdx;
		}
	}

	template<class TVoxel>
	__global__ void cleanMemory_device(int *voxelAllocationList, int *noAllocatedVoxelEntries, ITMHashSwapState *swapStates,
	                                   HashEntry *hashTable, TVoxel *localVBA, int *neededEntryIDs_local,
	                                   int neededEntryCount, const int voxelBlockCount)
	{
		int locId = threadIdx.x + blockIdx.x * blockDim.x;

		if (locId >= neededEntryCount) return;

		int entryDestId = neededEntryIDs_local[locId];

		swapStates[entryDestId].state = 0;

		int vbaIdx = atomicAdd(&noAllocatedVoxelEntries[0], 1);
		if (vbaIdx < voxelBlockCount - 1)
		{
			voxelAllocationList[vbaIdx + 1] = hashTable[entryDestId].ptr;
			hashTable[entryDestId].ptr = -1;
		}
	}

	__global__ void buildListToClean_device(int *neededEntryIDs, int *noNeededEntries, HashEntry *hashTable, HashBlockVisibility* blockVisibilityTypes, int noTotalEntries)
	{
		int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
		if (targetIdx > noTotalEntries - 1) return;

		__shared__ bool shouldPrefix;

		shouldPrefix = false;
		__syncthreads();

		bool isNeededId = hashTable[targetIdx].ptr >= 0 && blockVisibilityTypes[targetIdx] == 0;

		if (isNeededId) shouldPrefix = true;
		__syncthreads();

		if (shouldPrefix)
		{
			int offset = computePrefixSum_device<int>(isNeededId, noNeededEntries, blockDim.x * blockDim.y, threadIdx.x);
			if (offset != -1 && offset < SWAP_OPERATION_BLOCK_COUNT) neededEntryIDs[offset] = targetIdx;
		}
	}

	template<class TVoxel>
	__global__ void cleanMemory_device(int *voxelAllocationList, int *noAllocatedVoxelEntries,
	                                   HashEntry *hashTable, TVoxel *localVBA, int *neededEntryIDs_local,
	                                   int neededEntryCount, const int voxelBlockCount)
	{
		int locId = threadIdx.x + blockIdx.x * blockDim.x;

		if (locId >= neededEntryCount) return;

		int entryDestId = neededEntryIDs_local[locId];

		int vbaIdx = atomicAdd(&noAllocatedVoxelEntries[0], 1);
		if (vbaIdx < voxelBlockCount - 1)
		{
			voxelAllocationList[vbaIdx + 1] = hashTable[entryDestId].ptr;
			hashTable[entryDestId].ptr = -2;
		}
	}

	template<class TVoxel>
	__global__ void cleanVBA(int *neededEntryIDs_local, HashEntry *hashTable, TVoxel *localVBA)
	{
		int entryDestId = neededEntryIDs_local[blockIdx.x];

		HashEntry &hashEntry = hashTable[entryDestId];

		TVoxel *srcVB = localVBA + hashEntry.ptr * VOXEL_BLOCK_SIZE3;

		int vIdx = threadIdx.x + threadIdx.y * VOXEL_BLOCK_SIZE + threadIdx.z * VOXEL_BLOCK_SIZE * VOXEL_BLOCK_SIZE;
		srcVB[vIdx] = TVoxel();
	}

	template<class TVoxel>
	__global__ void moveActiveDataToTransferBuffer_device(TVoxel *syncedVoxelBlocks_local, bool *hasSyncedData_local,
	                                                      int *neededEntryIDs_local, HashEntry *hashTable, TVoxel *localVBA)
	{
		int entryDestId = neededEntryIDs_local[blockIdx.x];

		HashEntry &hashEntry = hashTable[entryDestId];

		TVoxel *dstVB = syncedVoxelBlocks_local + blockIdx.x * VOXEL_BLOCK_SIZE3;
		TVoxel *srcVB = localVBA + hashEntry.ptr * VOXEL_BLOCK_SIZE3;

		int vIdx = threadIdx.x + threadIdx.y * VOXEL_BLOCK_SIZE + threadIdx.z * VOXEL_BLOCK_SIZE * VOXEL_BLOCK_SIZE;
		dstVB[vIdx] = srcVB[vIdx];
		srcVB[vIdx] = TVoxel();

		if (vIdx == 0) hasSyncedData_local[blockIdx.x] = true;
	}

	template<class TVoxel>
	__global__ void integrateOldIntoActiveData_device(TVoxel *localVBA, ITMHashSwapState *swapStates, TVoxel *syncedVoxelBlocks_local,
	                                                  int *neededEntryIDs_local, HashEntry *hashTable, int maxW)
	{
		int entryDestId = neededEntryIDs_local[blockIdx.x];

		TVoxel *srcVB = syncedVoxelBlocks_local + blockIdx.x * VOXEL_BLOCK_SIZE3;
		TVoxel *dstVB = localVBA + hashTable[entryDestId].ptr * VOXEL_BLOCK_SIZE3;

		int vIdx = threadIdx.x + threadIdx.y * VOXEL_BLOCK_SIZE + threadIdx.z * VOXEL_BLOCK_SIZE * VOXEL_BLOCK_SIZE;

		CombineVoxelInformation<TVoxel::hasColorInformation, TVoxel>::compute(srcVB[vIdx], dstVB[vIdx], maxW);

		if (vIdx == 0) swapStates[entryDestId].state = 2;
	}

}
