//  ================================================================
//  Created by Gregory Kramida on 10/1/19.
//  Copyright (c) 2019 Gregory Kramida
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.
//  ================================================================

#include <vector>
#include "VolumeStatisticsCalculator_CUDA.h"
#include "../../../Configuration.h"
#include "../../../../Engines/Traversal/CUDA/VolumeTraversal_CUDA_VoxelBlockHash.h"
#include "../../../../Engines/Traversal/CUDA/VolumeTraversal_CUDA_PlainVoxelArray.h"
#include "../../../../Engines/Traversal/Interface/HashTableTraversal.h"
#include "../Shared/VolumeStatisticsCalculator_Functors.h"

using namespace ITMLib;

namespace {
//CUDA kernel declaration

__global__ void computeVoxelBounds(const HashEntry* hash_table, Vector6i* bounds, int hash_entry_count);
__global__ void computeHashBlockCount(const HashEntry* hashTable, int noTotalEntries, int* count);

} // end anonymous namespace

template<typename TVoxel, typename TIndex>
struct ComputeVoxelBoundsFunctor;

template<typename TVoxel>
struct ComputeVoxelBoundsFunctor<TVoxel, VoxelBlockHash> {
	static Vector6i Compute(const VoxelVolume<TVoxel, VoxelBlockHash>* volume) {

		Vector6i bounds = Vector6i(0);
		const TVoxel* voxelBlocks = volume->localVBA.GetVoxelBlocks();
		const HashEntry* hashTable = volume->index.GetEntries();
		int noTotalEntries = volume->index.hashEntryCount;

		dim3 cudaBlockSize(256, 1);
		dim3 cudaGridSize((int) ceil((float) noTotalEntries / (float) cudaBlockSize.x));

		Vector6i* boundsCuda = nullptr;

		ORcudaSafeCall(cudaMalloc((void**) &boundsCuda, sizeof(Vector6i)));
		ORcudaSafeCall(cudaMemcpy(boundsCuda, (void*) &bounds, sizeof(Vector6i), cudaMemcpyHostToDevice));

		computeVoxelBounds << < cudaGridSize, cudaBlockSize >> > (hashTable, boundsCuda, noTotalEntries);
		ORcudaKernelCheck;

		ORcudaSafeCall(cudaMemcpy((void*) &bounds, boundsCuda, sizeof(Vector6i), cudaMemcpyDeviceToHost));
		ORcudaSafeCall(cudaFree(boundsCuda));
		return bounds;
	}
};

template<typename TVoxel>
struct ComputeVoxelBoundsFunctor<TVoxel, PlainVoxelArray> {
	static Vector6i Compute(const VoxelVolume<TVoxel, PlainVoxelArray>* volume) {
		const PlainVoxelArray::IndexData* indexData = volume->index.GetIndexData();
		return Vector6i(indexData->offset.x, indexData->offset.y, indexData->offset.z,
		                indexData->offset.x + indexData->size.x, indexData->offset.y + indexData->size.y,
		                indexData->offset.z + indexData->size.z);
	}
};


template<typename TVoxel, typename TIndex>
Vector6i
VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeVoxelBounds(const VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeVoxelBoundsFunctor<TVoxel, TIndex>::Compute(volume);
}

template<typename TVoxel, typename TIndex>
int
VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeAllocatedVoxelCount(VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeAllocatedVoxelCountFunctor<TVoxel, TIndex, MEMORYDEVICE_CUDA>::compute(volume);
}


template<typename TVoxel, typename TIndex>
int VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeAllocatedHashBlockCount(
		VoxelVolume<TVoxel, TIndex>* volume) {
	return HashOnlyStatisticsFunctor<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeAllocatedHashBlockCount(volume);
}

template<typename TVoxel, typename TIndex>
std::vector<int>
VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::GetAllocatedHashCodes(VoxelVolume<TVoxel, TIndex>* volume) {
	return HashOnlyStatisticsFunctor<TVoxel, TIndex, MEMORYDEVICE_CUDA>::GetAllocatedHashCodes(volume);
}

template<typename TVoxel, typename TIndex>
std::vector<Vector3s>
VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::GetAllocatedHashBlockPositions(VoxelVolume<TVoxel, TIndex>* volume) {
	return HashOnlyStatisticsFunctor<TVoxel, TIndex, MEMORYDEVICE_CUDA>::GetAllocatedBlockPositions(volume);
}


template<bool hasSemanticInformation, typename TVoxel, typename TIndex, MemoryDeviceType TMemoryDeviceType>
struct ComputeNonTruncatedVoxelCountFunctor;

template<class TVoxel, typename TIndex>
struct ComputeNonTruncatedVoxelCountFunctor<false, TVoxel, TIndex> {
	static int compute(VoxelVolume<TVoxel, TIndex>* volume) {
		DIEWITHEXCEPTION("Voxels need to have semantic information to be marked as truncated or non-truncated.");
		return 0;
	}
};
template<class TVoxel, typename TIndex>
struct ComputeNonTruncatedVoxelCountFunctor<true, TVoxel, TIndex, MEMORYDEVICE_CUDA> {
	//not an optimal way (reduction would be much faster), but works -- we don't need this uber-fast
	__host__
	static int compute(VoxelVolume<TVoxel, TIndex>* volume) {
		ComputeNonTruncatedVoxelCountFunctor instance;

		VolumeTraversalEngine<TVoxel, TIndex, MEMORYDEVICE_CUDA>::
		VoxelTraversal(volume, instance);
		int count = 0;
		cudaMemcpy(&count, instance.count_device, sizeof(int), cudaMemcpyDeviceToHost);
		return count;
	}

	int* count_device = nullptr;
	__device__
	void operator()(TVoxel& voxel) {
		atomicAdd(count_device, (int) (voxel.flags == ITMLib::VOXEL_NONTRUNCATED));
	}

	ComputeNonTruncatedVoxelCountFunctor() {
		ORcudaSafeCall(cudaMalloc((void**) &count_device, sizeof(int)));
		ORcudaSafeCall(cudaMemset(count_device, 0, sizeof(int)));
	}

	~ComputeNonTruncatedVoxelCountFunctor() {
		ORcudaSafeCall(cudaFree(count_device));
	}
};

template<typename TVoxel, typename TIndex>
int VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeNonTruncatedVoxelCount(
		VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeNonTruncatedVoxelCountFunctor<TVoxel::hasSemanticInformation, TVoxel, TIndex>::compute(volume);
}

template<typename TVoxel, typename TIndex>
unsigned int VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::CountVoxelsWithSpecificSdfValue(
		VoxelVolume<TVoxel, TIndex>* volume, float value) {
	return ComputeVoxelCountWithSpecificValue<TVoxel::hasSDFInformation, TVoxel, TIndex, MEMORYDEVICE_CUDA>::compute(
			volume, value);
}

template<typename TVoxel, typename TIndex>
double VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeNonTruncatedVoxelAbsSdfSum(
		VoxelVolume<TVoxel, TIndex>* volume) {
	return SumSDFFunctor<TVoxel::hasSemanticInformation, TVoxel, TIndex, MEMORYDEVICE_CUDA>::compute(volume,
	                                                                              VoxelFlags::VOXEL_NONTRUNCATED);
}

template<typename TVoxel, typename TIndex>
double VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeTruncatedVoxelAbsSdfSum(
		VoxelVolume<TVoxel, TIndex>* volume) {
	return SumSDFFunctor<TVoxel::hasSemanticInformation, TVoxel, TIndex, MEMORYDEVICE_CUDA>::compute(volume, VoxelFlags::VOXEL_TRUNCATED);
}


template<typename TVoxel, typename TIndex>
float VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::FindMaxGradient0LengthAndPosition(
		VoxelVolume<TVoxel, TIndex>* volume, Vector3i& positionOut) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
	return 0.0f;
}

template<typename TVoxel, typename TIndex>
float VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::FindMaxGradient1LengthAndPosition(
		VoxelVolume<TVoxel, TIndex>* volume, Vector3i& positionOut) {
	DIEWITHEXCEPTION_REPORTLOCATION("Not implemented");
	return 0.0f;
}

template<typename TVoxel, typename TIndex>
unsigned int
VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeAlteredVoxelCount(VoxelVolume<TVoxel, TIndex>* volume) {
	IsAlteredCountFunctor<TVoxel> functor;
	VolumeTraversalEngine<TVoxel, TIndex, MEMORYDEVICE_CUDA>::VoxelTraversal(volume, functor);
	return functor.GetCount();
}

template<typename TVoxel, typename TIndex>
double VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeFramewiseWarpMin(VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeFramewiseWarpLengthStatisticFunctor<TVoxel::hasFramewiseWarp, TVoxel, TIndex, MEMORYDEVICE_CUDA, MINIMUM>::compute(
			volume);
}


template<typename TVoxel, typename TIndex>
double VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeFramewiseWarpMax(VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeFramewiseWarpLengthStatisticFunctor<TVoxel::hasFramewiseWarp, TVoxel, TIndex, MEMORYDEVICE_CUDA, MAXIMUM>::compute(
			volume);
}

template<typename TVoxel, typename TIndex>
double VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::ComputeFramewiseWarpMean(VoxelVolume<TVoxel, TIndex>* volume) {
	return ComputeFramewiseWarpLengthStatisticFunctor<TVoxel::hasFramewiseWarp, TVoxel, TIndex, MEMORYDEVICE_CUDA, MEAN>::compute(
			volume);
}

template<typename TVoxel, typename TIndex>
Extent3D VolumeStatisticsCalculator<TVoxel, TIndex, MEMORYDEVICE_CUDA>::FindMinimumNonTruncatedBoundingBox(
		VoxelVolume<TVoxel, TIndex>* volume) {
	return FlagMatchBBoxFunctor<TVoxel::hasSemanticInformation, TVoxel, TIndex, MEMORYDEVICE_CUDA>::
	compute(volume, VoxelFlags::VOXEL_NONTRUNCATED);
}

namespace {
// CUDA kernel implementations

__global__ void computeVoxelBounds(const HashEntry* hash_table, Vector6i* bounds, int hash_entry_count) {
	int hash = threadIdx.x + blockIdx.x * blockDim.x;
	if (hash >= hash_entry_count) return;

	const HashEntry& hashEntry = hash_table[hash];
	if (hashEntry.ptr < 0) return;

	Vector3i hashEntryPosVoxels = hashEntry.pos.toInt() * VOXEL_BLOCK_SIZE;

	atomicMin(&(bounds->min_x), hashEntryPosVoxels.x);
	atomicMin(&(bounds->min_y), hashEntryPosVoxels.y);
	atomicMin(&(bounds->min_z), hashEntryPosVoxels.z);
	atomicMax(&(bounds->max_x), hashEntryPosVoxels.x + VOXEL_BLOCK_SIZE);
	atomicMax(&(bounds->max_y), hashEntryPosVoxels.y + VOXEL_BLOCK_SIZE);
	atomicMax(&(bounds->max_z), hashEntryPosVoxels.z + VOXEL_BLOCK_SIZE);
}
} // end anonymous namespace
